{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Spam Email Classification: A Practical Approach to Binary Classification\n",
        "\n",
        "Email is one of the most widely used communication tools today, but it is also a common target for unsolicited and harmful messages. Detecting spam emails is an essential task to enhance privacy and improve user experience. This project focuses on classifying emails as either spam or legitimate (ham) using a combined dataset derived from two renowned sources: the **2007 TREC Public Spam Corpus** and the **Enron-Spam Dataset**.\n",
        "\n",
        "### About the Dataset\n",
        "\n",
        "The dataset used in this project consists of **83,446 email records** labeled as either:\n",
        "- **Spam (`1`)**: Unsolicited or harmful messages.\n",
        "- **Ham (`0`)**: Legitimate email content.\n",
        "\n",
        "Each record includes:\n",
        "1. **Label**: Indicates whether the email is spam or not.\n",
        "2. **Text**: The actual content of the email.\n",
        "\n",
        "### Data Sources\n",
        "\n",
        "The dataset combines information from:\n",
        "- [2007 TREC Public Spam Corpus](https://plg.uwaterloo.ca/~gvcormac/treccorpus07/)  \n",
        "  Preprocessed dataset: [Download here](https://www.kaggle.com/datasets/bayes2003/emails-for-spam-or-ham-classification-trec-2007)\n",
        "- [Enron-Spam Dataset](https://www2.aueb.gr/users/ion/data/enron-spam/)  \n",
        "  Preprocessed dataset: [Download here](https://github.com/MWiechmann/enron_spam_data/)\n",
        "\n",
        "The combination and preprocessing of these datasets were accomplished using a custom script available [here](https://github.com/PuruSinghvi/Spam-Email-Classifier/blob/main/Combining%20Datasets.ipynb).\n",
        "\n",
        "### Objective and Inspiration\n",
        "\n",
        "This project tackles a **binary classification problem** where the goal is to differentiate between spam and ham emails. The task involves understanding the nuances of email content and leveraging machine learning models to achieve high classification accuracy. The approach draws inspiration from Ramya Vidiyalaâ€™s article, [\"Detecting Spam in Emails\"](https://towardsdatascience.com/spam-detection-in-emails-de0398ea3b48), which highlights effective methodologies for spam detection.\n",
        "\n",
        "By identifying spam emails with high accuracy, this project aims to demonstrate the potential of machine learning in solving real-world challenges, such as improving email security and reducing unwanted communications.\n"
      ],
      "metadata": {
        "id": "uQHAlY0fBy2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "To prepare the dataset for spam email classification, the following libraries and tools are imported:  \n",
        "\n",
        "- **NumPy**: For numerical operations.  \n",
        "- **Pandas**: For data manipulation and handling CSV files.  \n",
        "- **NLTK (Natural Language Toolkit)**: For natural language processing tasks, including tokenization and stopword removal.  \n",
        "- **Regular Expressions (`re`)**: For text cleaning and pattern matching.  \n",
        "\n",
        "Additionally, necessary NLTK resources such as stopwords and tokenizers are downloaded to enable effective text processing.\n"
      ],
      "metadata": {
        "id": "CTrJwsTHBy-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ngrams\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JSe9CKgBzdc",
        "outputId": "070ef0e6-b72b-4497-8db8-ae8ed420289f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization\n",
        "\n",
        "For exploring and visualizing the dataset, the following libraries and tools are utilized:  \n",
        "\n",
        "- **Matplotlib**: For creating static, animated, and interactive visualizations.  \n",
        "- **Collections (Counter)**: For counting occurrences of elements in the dataset, such as word frequencies.  \n",
        "- **WordCloud**: For generating word cloud representations to visualize common terms in the dataset."
      ],
      "metadata": {
        "id": "497ReSJaB-xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from wordcloud import WordCloud"
      ],
      "metadata": {
        "id": "wNpN12MbB-9G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering\n",
        "\n",
        "To transform the dataset into a format suitable for machine learning models, the following libraries and tools are employed:  \n",
        "\n",
        "- **String**: For handling string operations and text processing.  \n",
        "- **Regular Expressions (`re`)**: For pattern matching and text cleaning.  \n",
        "- **Keras Preprocessing**:  \n",
        "  - **Tokenizer**: For converting text into sequences of tokens.  \n",
        "  - **Pad Sequences**: For ensuring uniform input length by padding or truncating sequences.  \n",
        "- **Scikit-learn Preprocessing**:  \n",
        "  - **LabelEncoder**: For encoding target labels (spam or ham) into numerical format.  "
      ],
      "metadata": {
        "id": "-PLOqG7jCAkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "import string\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "OQE3-6JaCAuE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Model\n",
        "\n",
        "The following libraries and tools are used to build, train, and evaluate the machine learning model:  \n",
        "\n",
        "- **Scikit-learn**:  \n",
        "  - **Train-Test Split**: For dividing the dataset into training and testing sets.  \n",
        "\n",
        "- **Keras**:  \n",
        "  - **Sequential**: For creating a linear stack of layers for the model.  \n",
        "  - **Layers**:  \n",
        "    - **Dense**: Fully connected layers for learning complex representations.  \n",
        "    - **LSTM**: Long Short-Term Memory layers for capturing sequential patterns in text data.  \n",
        "    - **Embedding**: For converting words into dense vector representations.  \n",
        "    - **Dropout**: For regularization to reduce overfitting.  \n",
        "    - **Activation**: For applying activation functions like ReLU or softmax.  \n",
        "    - **Bidirectional**: For processing sequences in both forward and backward directions.  \n",
        "\n",
        "- **TensorFlow**:  \n",
        "  - As the backend for training and deploying the neural network model."
      ],
      "metadata": {
        "id": "f6zs4cBICDJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Machine Learning Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Dropout, Activation, Bidirectional\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "SJq5o14CCDS6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metric\n",
        "\n",
        "To assess the performance of the machine learning model, the following libraries and tools are used:  \n",
        "\n",
        "- **Scikit-learn Metrics**:  \n",
        "  - **Confusion Matrix**: For visualizing true positives, true negatives, false positives, and false negatives.  \n",
        "  - **F1 Score**: For evaluating the balance between precision and recall.  \n",
        "  - **Precision Score**: For measuring the proportion of correctly identified positive instances.  \n",
        "  - **Recall Score**: For measuring the proportion of actual positives correctly identified.  \n",
        "\n",
        "- **Seaborn**:  \n",
        "  - For creating visually appealing and informative plots, such as heatmaps for the confusion matrix.  "
      ],
      "metadata": {
        "id": "kQASDPDsCIBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Metric\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "OKNeHmNsCIaf"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}